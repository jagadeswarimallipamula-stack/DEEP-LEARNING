import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import reuters
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

print(f"Number of training samples: {len(train_data)}")
print(f"Number of test samples: {len(test_data)}")
def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1
    return results

X_train = vectorize_sequences(train_data)
X_test = vectorize_sequences(test_data)
from tensorflow.keras.utils import to_categorical

y_train = to_categorical(train_labels)
y_test = to_categorical(test_labels)
model = Sequential([
    Dense(64, activation='relu', input_shape=(10000,)),
    Dense(64, activation='relu'),
    Dense(46, activation='softmax')  # 46 categories => softmax
])
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
history = model.fit(X_train, y_train,
                    epochs=10,
                    batch_size=512,
                    validation_split=0.2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title("Training vs Validation Accuracy")
plt.show()
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"\nTest Accuracy: {test_acc:.4f}")
predictions = model.predict(X_test)
predicted_class = np.argmax(predictions[0])
print(f"Predicted class for first test sample: {predicted_class}")
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
# Predict class probabilities
y_pred_probs = model.predict(X_test)

# Convert probabilities to predicted class index
y_pred = np.argmax(y_pred_probs, axis=1)

# Convert one-hot labels back to class index
y_true = np.argmax(y_test, axis=1)
# Count frequency of each true label
(unique, counts) = np.unique(y_true, return_counts=True)
sorted_indices = np.argsort(-counts)  # descending order
top_classes = unique[sorted_indices[:10]]

# Filter predictions and labels to only top 10 classes
mask = np.isin(y_true, top_classes)
y_true_top10 = y_true[mask]
y_pred_top10 = y_pred[mask]
# Build confusion matrix
cm_top10 = confusion_matrix(y_true_top10, y_pred_top10, labels=top_classes)

# Normalize (row-wise)
cm_top10_normalized = cm_top10.astype('float') / cm_top10.sum(axis=1)[:, np.newaxis]

# Plot with seaborn
plt.figure(figsize=(10, 8))
sns.heatmap(cm_top10_normalized, annot=True, fmt=".2f", cmap="Blues",
            xticklabels=top_classes, yticklabels=top_classes)
plt.title("Normalized Confusion Matrix (Top 10 Reuters Classes)")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.tight_layout()
plt.show()
model.save("reuters_news_model.h5")
from tensorflow.keras.models import load_model
model = load_model("reuters_news_model.h5")
# Get the mapping from word -> index
word_index = reuters.get_word_index()
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
def encode_new_text(text, word_index, max_words=10000):
    # Basic preprocessing: lowercase and split
    tokens = text.lower().split()
    # Convert to word indices (0 for unknown)
    encoded = [word_index.get(word, 0) for word in tokens]
    # Create binary bag-of-words vector
    vectorized = np.zeros((1, max_words))
    for index in encoded:
        if index < max_words:
            vectorized[0, index] = 1
    return vectorized
new_text = "The stock market saw a major shift as tech companies surged in the second quarter"
x_input = encode_new_text(new_text, word_index)

prediction = model.predict(x_input)
predicted_class = np.argmax(prediction[0])

print(f"Predicted topic class: {predicted_class} (Confidence: {np.max(prediction[0]):.4f})")
from tensorflow.keras.datasets import reuters
_, train_labels = reuters.load_data(num_words=10000)[0]
num_classes = np.max(train_labels) + 1
print("Number of classes:", num_classes)
